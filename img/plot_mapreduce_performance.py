import pandas as pd
import matplotlib.pyplot as plt
import glob
import os
import re

# ========================
# ç”¨æˆ·é…ç½®åŒº
# ========================
INPUT_DIR = "MapReduce_S2"
# dstat CSV æ–‡ä»¶åçš„åŒ¹é…æ¨¡å¼ï¼Œä¾‹å¦‚ "metrics_node1.csv", "metrics_node2.csv"
CSV_PATTERN = "metrics_*.csv"
# åŒ…å« MapReduce è¿­ä»£æ—¶é—´çš„æ€§èƒ½æŠ¥å‘Šæ–‡ä»¶å
PERFORMANCE_REPORT = "performance_report.txt"

# å›¾è¡¨å’Œç»Ÿè®¡ç»“æœçš„è¾“å‡ºç›®å½•
OUTPUT_DIR = "MapReduce_S2_Figure"

# æ‚¨å¸Œæœ›åˆ†æçš„ MapReduce è¿­ä»£è½®æ•°
NUM_ITERATIONS = 10


# --- åˆå§‹åŒ– ---
os.makedirs(OUTPUT_DIR, exist_ok=True)

# è®¾ç½®å›¾è¡¨é»˜è®¤æ ·å¼
plt.rcParams.update({
    "figure.figsize": (14, 7),
    "font.size": 11,
    "axes.grid": True,
    "grid.alpha": 0.5
})

# dstat CSV æ–‡ä»¶ä¸­çš„åˆ—å
COLUMNS = [
    "epoch", "usr", "sys", "idl", "wai", "hiq", "siq",
    "read", "writ",
    "recv", "send",
    "used", "buff", "cach", "free"
]


def parse_performance_report(filepath, num_iterations):
    """
    è§£ææ€§èƒ½æŠ¥å‘Šï¼Œæå–å‰ N è½®çš„ "Total Iteraction Time" å¹¶è®¡ç®—æ€»æ—¶é•¿ã€‚
    """
    if not os.path.exists(filepath):
        raise FileNotFoundError(f"æ€§èƒ½æŠ¥å‘Šæ–‡ä»¶æœªæ‰¾åˆ°: {filepath}")

    iteration_times_ms = []

    with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
        for line in f:
            line = line.strip()
            # åŒ¹é…: Iteration_1_Total Iteraction Time       :      94555 ms
            match = re.search(r'Iteration_(\d+)_Total\s+Iteraction\s+Time\s*:\s*(\d+)\s*ms', line)
            if match:
                iter_num = int(match.group(1))
                time_ms = int(match.group(2))
                if iter_num <= num_iterations:
                    # ä½¿ç”¨åˆ—è¡¨ç´¢å¼•å¯¹é½ï¼ˆiter_num ä» 1 å¼€å§‹ï¼‰
                    while len(iteration_times_ms) < iter_num:
                        iteration_times_ms.append(None)
                    iteration_times_ms[iter_num - 1] = time_ms
                if len([t for t in iteration_times_ms if t is not None]) >= num_iterations:
                    break

    valid_times = [t for t in iteration_times_ms if t is not None][:num_iterations]

    if not valid_times:
        raise ValueError(
            "åœ¨æ€§èƒ½æŠ¥å‘Šä¸­æœªæ‰¾åˆ° 'Iteration_X_Total Iteraction Time' æ¡ç›®ï¼\n"
            "è¯·ç¡®ä¿æŠ¥å‘Šä¸­åŒ…å«ç±»ä¼¼ä»¥ä¸‹çš„è¡Œ:\n"
            "  Iteration_1_Total Iteraction Time       :      94555 ms"
        )

    total_ms = sum(valid_times)
    total_sec = total_ms / 1000.0
    print(f"âœ… æˆåŠŸä»æŠ¥å‘Šä¸­è§£æäº† {len(valid_times)} è½®è¿­ä»£ã€‚")
    print(f"â±ï¸  å‰ {num_iterations} è½®è¿­ä»£æ€»è€—æ—¶: {total_sec:.3f} ç§’")
    return total_sec


def read_dstat_csv(filepath):
    """
    è¯»å–å¹¶é¢„å¤„ç†å•ä¸ª dstat CSV æ–‡ä»¶ã€‚
    """
    df = pd.read_csv(
        filepath,
        skiprows=5,
        header=None,
        names=COLUMNS,
        on_bad_lines='skip',
        engine='python'
    )

    for col in df.columns:
        df[col] = pd.to_numeric(df[col], errors='coerce')

    df = df.dropna(subset=['epoch'])
    df = df.fillna(0)
    df['time'] = pd.to_datetime(df['epoch'], unit='s', origin='unix')

    base_name = os.path.basename(filepath).replace("metrics_", "").replace(".csv", "")
    node_name = base_name.split('(')[0]

    return df, node_name


# ========================
# æ­¥éª¤ 1: è§£ææ€§èƒ½æŠ¥å‘Šï¼Œè·å–å‰ N è½®æ€»æ—¶é•¿
# ========================
TOTAL_DURATION_SEC = parse_performance_report(PERFORMANCE_REPORT, NUM_ITERATIONS)

# ========================
# æ­¥éª¤ 2: åŠ è½½æ‰€æœ‰ dstat æ•°æ®
# ========================
all_data = {}
for f in glob.glob(CSV_PATTERN):
    try:
        df, node = read_dstat_csv(f)
        all_data[node] = df
        print(f"âœ… å·²åŠ è½½ {f} ({len(df)} æ¡æ ·æœ¬)")
    except Exception as e:
        print(f"âŒ è·³è¿‡ {f}: {e}")

if not all_data:
    raise RuntimeError("æœªæ‰¾åˆ°æœ‰æ•ˆçš„ dstat æ•°æ®ï¼")

# ========================
# æ­¥éª¤ 3: åŸºäºç³»ç»Ÿæ´»åŠ¨æ£€æµ‹ä»»åŠ¡å®é™…å¼€å§‹æ—¶é—´
# ========================
activity_start_epochs = []
for df in all_data.values():
    # å½“ CPU ä½¿ç”¨ç‡ > 5% æˆ–ç£ç›˜å†™å…¥ > 1MB/s æ—¶ï¼Œè®¤ä¸ºä»»åŠ¡å¼€å§‹
    cpu_active = (100 - df['idl']) > 5
    disk_active = df['writ'] > (1 * 1024 * 1024)
    active = cpu_active | disk_active

    if active.any():
        first_active_epoch = df.loc[active.idxmax(), 'epoch']
        activity_start_epochs.append(first_active_epoch)

if not activity_start_epochs:
    task_start_epoch = min(df['epoch'].min() for df in all_data.values())
else:
    task_start_epoch = min(activity_start_epochs)

task_end_epoch = task_start_epoch + TOTAL_DURATION_SEC

print(f"ğŸ¯ æ£€æµ‹åˆ°ä»»åŠ¡å¼€å§‹äº: {pd.to_datetime(task_start_epoch, unit='s')}")
print(f"ğŸ”š {NUM_ITERATIONS} è½®è¿­ä»£åç»“æŸäº: {pd.to_datetime(task_end_epoch, unit='s')}")

# ========================
# æ­¥éª¤ 4: æ ¹æ®ä»»åŠ¡èµ·æ­¢æ—¶é—´è£å‰ªæ•°æ®
# ========================
filtered_data = {}
for node, df in all_data.items():
    mask = (df['epoch'] >= task_start_epoch) & (df['epoch'] <= task_end_epoch)
    filtered_df = df[mask].copy()

    # --- ä¸ºæ–°æŒ‡æ ‡è®¡ç®—åšå¥½å‡†å¤‡ ---
    # è®¡ç®—æ€»å†…å­˜ï¼Œé¿å…åç»­é‡å¤è®¡ç®—
    filtered_df['total_mem'] = filtered_df['used'] + filtered_df['buff'] + filtered_df['cach'] + filtered_df['free']

    filtered_data[node] = filtered_df
    print(f"ğŸ“Š å·²è¿‡æ»¤ {node}: ä¿ç•™ {len(filtered_df)} / {len(df)} æ¡æ ·æœ¬")


# ========================
# é€šç”¨ç»˜å›¾å‡½æ•°
# ========================
def plot_metric(title, ylabel, filename, get_y, legend_loc='upper right'):
    plt.figure()
    for node, df in filtered_data.items():
        if not df.empty:
            y_values = get_y(df)
            plt.plot(df['time'], y_values, label=node, linewidth=1.5)

    plt.title(title, fontsize=14)
    plt.xlabel("Time")
    plt.ylabel(ylabel)
    plt.legend(title="Node", loc=legend_loc)
    ax = plt.gca()
    ax.ticklabel_format(style='plain', axis='y')
    ax.set_ylim(bottom=0)  # Yè½´ä»0å¼€å§‹
    plt.tight_layout()
    plt.savefig(f"{OUTPUT_DIR}/{filename}", dpi=200)
    plt.close()
    print(f"âœ… å·²ä¿å­˜å›¾è¡¨: {filename}")


# ========================
# ç»˜åˆ¶è®ºæ–‡ä¸­å¯¹åº”çš„ä¸‰ä¸ªæ ¸å¿ƒæŒ‡æ ‡å›¾è¡¨
# ========================

# 1. CPU ä½¿ç”¨ç‡ (%)
plot_metric(
    f"Per-Node CPU Utilization (First {NUM_ITERATIONS} Iterations)",
    "CPU Usage (%)",
    "cpu_usage_percent.png",
    lambda df: df['usr']+df['sys'] #100 - df['idl']  # CPUä½¿ç”¨ç‡ = 100 - ç©ºé—²ç‡
)

# 2. CPU I/O ç­‰å¾…æ—¶é—´ (%)
plot_metric(
    f"Per-Node CPU I/O Wait Time (First {NUM_ITERATIONS} Iterations)",
    "CPU I/O Wait (%)",
    "cpu_io_wait_percent.png",
    lambda df: df['wai']  # 'wai' åˆ—æœ¬èº«å°±æ˜¯ I/O ç­‰å¾…æ—¶é—´çš„ç™¾åˆ†æ¯”
)

# 3. å†…å­˜ä½¿ç”¨ç‡ (%)
plot_metric(
    f"Per-Node Memory Utilization (First {NUM_ITERATIONS} Iterations)",
    "Memory Usage (%)",
    "memory_usage_percent.png",
    # å†…å­˜ä½¿ç”¨ç‡ = (å·²ç”¨å†…å­˜ / æ€»å†…å­˜) * 100
    # ä¸ºé¿å…é™¤ä»¥é›¶ï¼Œåœ¨åˆ†æ¯ä¸º0æ—¶è¿”å›0
    lambda df: (df['used']+df['buff'] / df['total_mem'] * 100).where(df['total_mem'] > 0, 0)
)

# ========================
# ç”Ÿæˆæ ¸å¿ƒæŒ‡æ ‡çš„ç»Ÿè®¡æ‘˜è¦
# ========================
stats = {}
for node, df in filtered_data.items():
    if not df.empty:
        stats[node] = {
            "CPU_Usage_avg(%)": (100 - df['idl']).mean(),
            "CPU_Usage_peak(%)": (100 - df['idl']).max(),
            "CPU_IOWait_avg(%)": df['wai'].mean(),
            "CPU_IOWait_peak(%)": df['wai'].max(),
            "Memory_Usage_avg(%)": ((df['used'] / df['total_mem'] * 100).where(df['total_mem'] > 0, 0)).mean(),
            "Memory_Usage_peak(%)": ((df['used'] / df['total_mem'] * 100).where(df['total_mem'] > 0, 0)).max(),
        }

if stats:
    stats_df = pd.DataFrame(stats).T
    stats_df.round(3).to_csv(f"{OUTPUT_DIR}/paper_metrics_summary.csv")

    print(f"\nğŸ“Š Performance Summary (First {NUM_ITERATIONS} Iterations):")
    print(stats_df.round(3))

    with open(f"{OUTPUT_DIR}/paper_metrics_summary.txt", "w") as f:
        f.write(f"=== Hadoop PageRank Performance Summary (First {NUM_ITERATIONS} Iterations) ===\n\n")
        f.write(stats_df.round(3).to_string())

    print(f"\nâœ… ç»Ÿè®¡æ‘˜è¦å·²ä¿å­˜è‡³: {OUTPUT_DIR}/paper_metrics_summary.txt and .csv")
else:
    print("\nâš ï¸ æœªç”Ÿæˆç»Ÿè®¡æ‘˜è¦ï¼Œå› ä¸ºæ²¡æœ‰ç¬¦åˆæ—¶é—´èŒƒå›´çš„æ•°æ®ã€‚")

print(f"\nğŸ‰ æ‰€æœ‰å›¾è¡¨å’Œç»Ÿè®¡æ•°æ®å·²ä¿å­˜è‡³ç›®å½• '{OUTPUT_DIR}'!")